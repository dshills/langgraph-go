# LangGraph-Go

A Go-native orchestration framework for building **stateful, graph-based LLM and tool workflows**.

> **âš ï¸ Alpha Software Warning**
>
> LangGraph-Go is currently in **alpha** stage. The API is not yet stable and may change significantly between releases. While the core functionality is complete and tested, we recommend:
> - **Not using in production** without thorough testing
> - **Pinning to specific versions** in your go.mod
> - **Expecting breaking changes** until v1.0.0
> - **Reporting issues** to help us improve stability
>
> We welcome early adopters and contributors! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.

## Overview

LangGraph-Go enables you to build complex AI agent systems with:
- **Deterministic replay** - Resume workflows from any checkpoint
- **Type-safe state** - Strongly-typed state management using Go generics
- **Flexible routing** - Conditional logic, loops, and parallel execution
- **LLM integration** - Unified interface for OpenAI, Anthropic, Google, and local models
- **Production-ready** - Built-in persistence, observability, and error handling

## Quick Start

```go
package main

import (
    "context"
    "fmt"
    "os"

    "github.com/dshills/langgraph-go/graph"
    "github.com/dshills/langgraph-go/graph/emit"
    "github.com/dshills/langgraph-go/graph/store"
)

// Define your state type
type Session struct {
    Query  string
    Answer string
    Steps  int
}

// Create a reducer for state merging
func reduce(prev, delta Session) Session {
    if delta.Query != "" {
        prev.Query = delta.Query
    }
    if delta.Answer != "" {
        prev.Answer = delta.Answer
    }
    prev.Steps += delta.Steps
    return prev
}

func main() {
    // Create nodes
    process := graph.NodeFunc[Session](func(ctx context.Context, s Session) graph.NodeResult[Session] {
        return graph.NodeResult[Session]{
            Delta: Session{Answer: "Processed: " + s.Query, Steps: 1},
            Route: graph.Stop(),
        }
    })

    // Build the workflow
    st := store.NewMemStore[Session]()
    emitter := emit.NewLogEmitter(os.Stdout, false)
    opts := graph.Options{MaxSteps: 10}

    engine := graph.New(reduce, st, emitter, opts)
    engine.Add("process", process)
    engine.StartAt("process")

    // Execute
    ctx := context.Background()
    final, err := engine.Run(ctx, "run-001", Session{Query: "Hello LangGraph!"})
    if err != nil {
        panic(err)
    }

    fmt.Println(final.Answer) // Output: Processed: Hello LangGraph!
}
```

## Installation

```bash
# Install latest version (may include breaking changes)
go get github.com/dshills/langgraph-go

# Recommended: Pin to a specific version
go get github.com/dshills/langgraph-go@v0.1.0
```

**Note:** Since this is alpha software, we recommend pinning to a specific version in your `go.mod` to avoid unexpected breaking changes.

## Core Concepts

### Nodes

Nodes are processing units that receive state, perform computation, and return state modifications:

```go
type Node[S any] interface {
    Run(ctx context.Context, state S) NodeResult[S]
}
```

### State & Reducers

State flows through the workflow and is merged using reducer functions:

```go
type Reducer[S any] func(prev S, delta S) S
```

### Routing

Control flow with conditional routing:

```go
// Explicit routing
graph.Goto("next-node")
graph.Stop()

// Conditional edges
engine.Connect("nodeA", "nodeB", func(s Session) bool {
    return s.Confidence > 0.8
})
```

### Persistence

Automatic checkpointing enables workflow resumption:

```go
// Save checkpoint
engine.SaveCheckpoint("after-step-1")

// Resume from checkpoint
engine.LoadCheckpoint("after-step-1")
final, err := engine.Run(ctx, runID, initialState)
```

### Concurrent Execution

**New in v0.2.0**: Execute independent nodes in parallel with deterministic results:

```go
// Enable concurrent execution with worker pool
opts := graph.Options{
    MaxConcurrentNodes: 10,  // Execute up to 10 nodes in parallel
    QueueDepth:         1000, // Frontier queue capacity
}

engine := graph.New(reducer, store, emitter, opts)

// Graph with parallel branches executes concurrently
// Fan-out: one node spawns multiple parallel branches
engine.Add("start", startNode)
engine.Add("branchA", branchANode) // Executes in parallel
engine.Add("branchB", branchBNode) // Executes in parallel
engine.Add("branchC", branchCNode) // Executes in parallel
engine.Add("merge", mergeNode)     // Results merge deterministically

// Results are deterministic regardless of completion order
final, err := engine.Run(ctx, runID, initialState)
```

**Key Features:**

- âš¡ **Performance**: 2-5x speedup for workflows with independent nodes
- ğŸ¯ **Deterministic**: Same results regardless of execution order
- ğŸ”„ **Replay**: Record and replay executions exactly for debugging
- ğŸ›¡ï¸ **Backpressure**: Automatic queue management prevents resource exhaustion
- ğŸ“Š **Observability**: Built-in metrics for queue depth and active workers

**Deterministic Replay** for debugging production issues:

```go
// Original execution automatically recorded
final, err := engine.Run(ctx, "prod-run-123", initialState)

// Load checkpoint from production
checkpoint, _ := store.LoadCheckpointV2(ctx, "prod-run-123", "")

// Replay execution exactly (no external API calls)
opts := graph.Options{ReplayMode: true}
replayResult, _ := engine.ReplayRun(ctx, "debug-replay", checkpoint)

// Identical results, full debugging context
```

See [Concurrency Guide](./docs/concurrency.md) and [Replay Guide](./docs/replay.md) for details.

## Architecture

```
/graph
  â”œâ”€â”€ engine.go      # Workflow execution engine
  â”œâ”€â”€ node.go        # Node abstractions
  â”œâ”€â”€ edge.go        # Edge and predicate types
  â”œâ”€â”€ state.go       # State management
  â”œâ”€â”€ store/         # Persistence layer
  â”‚   â”œâ”€â”€ store.go   # Store interface
  â”‚   â”œâ”€â”€ memory.go  # In-memory store (for testing)
  â”‚   â”œâ”€â”€ mysql.go   # MySQL/Aurora store
  â”‚   â””â”€â”€ mysql/     # MySQL implementation details
  â”œâ”€â”€ emit/          # Event system
  â”‚   â”œâ”€â”€ emitter.go # Emitter interface
  â”‚   â”œâ”€â”€ event.go   # Event types
  â”‚   â”œâ”€â”€ log.go     # Logging emitter
  â”‚   â”œâ”€â”€ buffered.go# Buffered emitter
  â”‚   â””â”€â”€ null.go    # Null emitter
  â”œâ”€â”€ model/         # LLM integrations
  â”‚   â”œâ”€â”€ chat.go    # ChatModel interface
  â”‚   â”œâ”€â”€ openai/    # OpenAI adapter
  â”‚   â”œâ”€â”€ anthropic/ # Anthropic Claude adapter
  â”‚   â”œâ”€â”€ google/    # Google Gemini adapter
  â”‚   â””â”€â”€ mock.go    # Mock for testing
  â””â”€â”€ tool/          # Tool abstractions
      â”œâ”€â”€ tool.go    # Tool interface
      â”œâ”€â”€ http.go    # HTTP tool implementation
      â””â”€â”€ mock.go    # Mock tool for testing
```

## Features

- âœ… **Stateful Execution** - Checkpoint and resume workflows
- âœ… **Conditional Routing** - Dynamic control flow based on state
- âœ… **Parallel Execution** - Fan-out to concurrent nodes
- âœ… **Concurrent Execution** - Worker pool with deterministic ordering (v0.2.0)
- âœ… **Deterministic Replay** - Record and replay executions exactly (v0.2.0)
- âœ… **LLM Integration** - OpenAI, Anthropic, Google Gemini
- âœ… **Tool Support** - HTTP tools and custom tool integration
- âœ… **Persistence** - MySQL/Aurora store for production use
- âœ… **Event Tracing** - Comprehensive observability with multiple emitters
- âœ… **Type Safety** - Go generics for compile-time safety
- âœ… **Production Ready** - Error handling, retries, timeouts, backpressure

## Examples

See the [`examples/`](./examples) directory for complete, runnable examples:

- **`chatbot/`** - Customer support chatbot with intent detection
- **`checkpoint/`** - Checkpoint and resume workflows
- **`routing/`** - Conditional routing based on state
- **`parallel/`** - Parallel execution with fan-out/fan-in
- **`llm/`** - Multi-provider LLM integration (OpenAI, Anthropic, Google)
- **`tools/`** - Tool calling and integration
- **`data-pipeline/`** - Data processing pipeline
- **`research-pipeline/`** - Multi-stage research workflow
- **`interactive-workflow/`** - Interactive user input workflow
- **`tracing/`** - Event tracing and observability
- **`benchmarks/`** - Performance benchmarking

All examples can be built with:
```bash
make examples
./build/<example-name>
```

## Documentation

### User Guides

- [Getting Started](./docs/guides/01-getting-started.md) - Build your first workflow
- [Building Workflows](./docs/guides/02-building-workflows.md) - Patterns and best practices
- [State Management](./docs/guides/03-state-management.md) - Advanced reducer patterns
- [Checkpoints & Resume](./docs/guides/04-checkpoints.md) - Save and resume workflows
- [Conditional Routing](./docs/guides/05-routing.md) - Dynamic control flow
- [Parallel Execution](./docs/guides/06-parallel.md) - Concurrent node execution
- [LLM Integration](./docs/guides/07-llm-integration.md) - Multi-provider LLM support
- [Event Tracing](./docs/guides/08-event-tracing.md) - Observability and monitoring

### Advanced Topics (v0.2.0)

- [Concurrency Model](./docs/concurrency.md) - Worker pool, ordering, backpressure
- [Deterministic Replay](./docs/replay.md) - Record and replay executions
- [Migration Guide v0.1â†’v0.2](./docs/migration-v0.2.md) - Upgrade from v0.1.x

### Reference

- [API Reference](./docs/api/) - Complete API documentation
- [FAQ](./docs/FAQ.md) - Frequently asked questions

### Project Documentation

- [Architecture Overview](./CLAUDE.md)
- [Technical Specification](./specs/SPEC.md)
- [Contributing Guide](./CONTRIBUTING.md)

## Development

The project includes a comprehensive Makefile for common development tasks:

```bash
# Build the library
make build

# Build all examples
make examples

# Run tests
make test

# Run tests with verbose output
make test-verbose

# Run tests with coverage report
make test-cover

# Run benchmarks
make bench

# Format code
make fmt

# Run go vet
make vet

# Run linter (if golangci-lint is available)
make lint

# Clean build artifacts
make clean

# Install dependencies
make install

# Build everything
make all

# Show all available targets
make help
```

Or use standard Go commands:

```bash
# Run tests
go test ./...

# Run tests with coverage
go test -cover ./...

# Run linter
golangci-lint run

# Build
go build ./...
```

## License

MIT License

## Project Status

**Current Version:** v0.1.0-alpha

âœ… **Core Framework Complete** - All 5 user stories have been implemented and tested:

- âœ… **US1**: Stateful workflow with checkpointing
- âœ… **US2**: Conditional routing and dynamic control flow
- âœ… **US3**: Parallel execution with fan-out/fan-in
- âœ… **US4**: Multi-provider LLM integration
- âœ… **US5**: Comprehensive event tracing and observability

**Current Phase**: Alpha release - API stabilization and community feedback

**Roadmap to v1.0.0:**
- Gather feedback from early adopters
- Stabilize public API based on real-world usage
- Address any critical bugs or issues
- Complete API documentation
- Add more examples and use cases

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for development guidelines including our Test-Driven Development workflow.

Please note that this project is released with a [Code of Conduct](./CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## Security

For security concerns, please see our [Security Policy](./SECURITY.md).

## Acknowledgments

This project is inspired by [LangGraph](https://github.com/langchain-ai/langgraph) by LangChain AI, a powerful framework for building stateful, multi-actor applications with LLMs. LangGraph-Go brings these concepts to the Go ecosystem, redesigned from the ground up to leverage Go's type system, generics, and concurrency primitives.

**Key differences from the original LangGraph:**
- **Type-safe** - Uses Go generics for compile-time type safety
- **Go-native** - Idiomatic Go design patterns and error handling
- **Concurrency-first** - Built on goroutines and channels for efficient parallelism
- **Minimal dependencies** - Pure Go core with optional external adapters

We are grateful to the LangChain AI team for pioneering the graph-based workflow approach for LLM applications. Check out the original Python implementation at https://github.com/langchain-ai/langgraph.
