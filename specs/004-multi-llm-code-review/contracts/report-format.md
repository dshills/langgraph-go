# Report Format Contract

## Overview

This document specifies the output format for the consolidated code review report generated by the Multi-LLM Code Review workflow.

## Markdown Report Format

The consolidated report is generated as a Markdown file with the following structure:

---

### Template

```markdown
# Code Review Report

**Codebase**: {codebase_root}
**Generated**: {timestamp}
**Duration**: {duration}
**Files Reviewed**: {total_files}
**Issues Found**: {total_issues}
**Providers**: {provider_list}

---

## Summary Statistics

| Metric | Value |
|--------|-------|
| Total Files Reviewed | {count} |
| Critical Issues | {count} |
| High Priority Issues | {count} |
| Medium Priority Issues | {count} |
| Low Priority Issues | {count} |
| Informational Issues | {count} |
| **Total Issues** | **{count}** |

### By Category

| Category | Count |
|----------|-------|
| Security | {count} |
| Performance | {count} |
| Best Practices | {count} |
| Style | {count} |

### Provider Statistics

| Provider | Issues Found | Tokens Used | Duration |
|----------|--------------|-------------|----------|
| OpenAI GPT-4 | {count} | {tokens} | {duration} |
| Anthropic Claude | {count} | {tokens} | {duration} |
| Google Gemini | {count} | {tokens} | {duration} |

---

## Critical Issues

{No critical issues found}

or

### 1. {issue_description} ðŸ“Œ

**File**: `{file_path}:{line_number}`
**Severity**: Critical
**Category**: {category}
**Providers**: {provider1}, {provider2}
**Consensus**: {2/3 providers}

**Description**:
{detailed_description}

**Remediation**:
{remediation_steps}

---

## High Priority Issues

{No high priority issues found}

or

### 2. {issue_description}

**File**: `{file_path}:{line_number}`
**Severity**: High
**Category**: {category}
**Providers**: {provider1}
**Consensus**: {1/3 providers}

**Description**:
{detailed_description}

**Remediation**:
{remediation_steps}

---

## Medium Priority Issues

{Similar structure...}

---

## Low Priority Issues

{Similar structure...}

---

## Informational Issues

{Similar structure...}

---

## Files Reviewed

### Clean Files (No Issues)
- `{file_path}`
- `{file_path}`

### Files with Issues
- `{file_path}` - {issue_count} issues
- `{file_path}` - {issue_count} issues

---

## Provider Details

### OpenAI GPT-4
- **Status**: Success âœ“
- **Issues Found**: {count}
- **Tokens Used**: {tokens}
- **Duration**: {duration}

### Anthropic Claude
- **Status**: Success âœ“
- **Issues Found**: {count}
- **Tokens Used**: {tokens}
- **Duration**: {duration}

### Google Gemini
- **Status**: Failed âœ—
- **Error**: Rate limit exceeded
- **Issues Found**: 0

---

## Review Configuration

- **Batch Size**: {batch_size}
- **Focus Areas**: {focus_areas}
- **Include Patterns**: {patterns}
- **Exclude Patterns**: {patterns}

---

Generated by Multi-LLM Code Review Workflow
```

---

## JSON Report Format

Alternative output format when `output.format: json` is specified in configuration.

```json
{
  "metadata": {
    "codebase_root": "/path/to/codebase",
    "generated_at": "2025-10-29T14:30:00Z",
    "duration_seconds": 120,
    "total_files_reviewed": 150,
    "total_issues_found": 42
  },
  "summary": {
    "by_severity": {
      "critical": 2,
      "high": 8,
      "medium": 15,
      "low": 12,
      "info": 5
    },
    "by_category": {
      "security": 10,
      "performance": 12,
      "best-practice": 15,
      "style": 5
    }
  },
  "providers": [
    {
      "name": "openai",
      "status": "success",
      "issues_found": 18,
      "tokens_used": 12500,
      "duration_ms": 45000
    },
    {
      "name": "anthropic",
      "status": "success",
      "issues_found": 20,
      "tokens_used": 15000,
      "duration_ms": 50000
    },
    {
      "name": "google",
      "status": "failed",
      "error": "Rate limit exceeded",
      "issues_found": 0,
      "tokens_used": 0,
      "duration_ms": 0
    }
  ],
  "issues": [
    {
      "id": "a3f2b8c1",
      "file": "main.go",
      "line": 42,
      "severity": "critical",
      "category": "security",
      "description": "SQL injection vulnerability: user input not sanitized",
      "remediation": "Use parameterized queries or prepared statements",
      "providers": ["openai", "anthropic"],
      "consensus_score": 0.67
    },
    {
      "id": "d9e1f7a4",
      "file": "handler.go",
      "line": 120,
      "severity": "high",
      "category": "performance",
      "description": "Inefficient loop: O(nÂ²) complexity",
      "remediation": "Use map for O(1) lookups instead of nested loops",
      "providers": ["openai"],
      "consensus_score": 0.33
    }
  ],
  "files_reviewed": [
    {
      "path": "main.go",
      "issue_count": 3,
      "status": "reviewed"
    },
    {
      "path": "handler.go",
      "issue_count": 5,
      "status": "reviewed"
    },
    {
      "path": "utils.go",
      "issue_count": 0,
      "status": "clean"
    }
  ],
  "configuration": {
    "batch_size": 20,
    "focus_areas": ["security", "performance", "best-practices"],
    "include_patterns": ["*.go", "*.py", "*.js"],
    "exclude_patterns": ["*_test.go", "vendor/**"]
  }
}
```

---

## Field Definitions

### Severity Levels (Ordered by Priority)

1. **critical**: Security vulnerabilities, data loss risks, crashes
2. **high**: Bugs that affect functionality, performance bottlenecks
3. **medium**: Code quality issues, maintainability concerns
4. **low**: Minor style issues, non-critical suggestions
5. **info**: Informational notes, documentation suggestions

### Category Definitions

- **security**: Security vulnerabilities, authentication/authorization issues, data exposure
- **performance**: Inefficient algorithms, memory leaks, slow queries
- **best-practice**: Violations of language/framework best practices, design patterns
- **style**: Code formatting, naming conventions, documentation

### Consensus Score

- Fraction of providers that identified the same issue (after deduplication)
- Range: 0.33 (1/3) to 1.0 (3/3)
- Higher consensus indicates more agreement across providers
- Issues with high consensus (â‰¥0.67) should be prioritized

---

## Report Generation Rules

1. **Issue Ordering**: Sort by severity (critical â†’ info), then by consensus score (high â†’ low)
2. **Deduplication**: Merge issues with same file, similar line (Â±5), similar description (Levenshtein < 30%)
3. **File Grouping**: Group issues by file for easy navigation
4. **Provider Attribution**: Always show which providers identified each issue
5. **Empty Sections**: Show "{No {severity} issues found}" for empty severity levels
6. **Links**: Use relative file paths for easy navigation in IDEs

---

## Output File Naming

- **Markdown**: `code-review-{timestamp}.md` (e.g., `code-review-2025-10-29T14-30-00.md`)
- **JSON**: `code-review-{timestamp}.json`
- **Checkpoint**: `.langgraph-checkpoint-{runID}.json` (hidden file)

All files written to `output.directory` specified in configuration.

---

## Example Usage

```bash
# Generate markdown report (default)
$ ./multi-llm-review --config config.yaml /path/to/codebase

# Generate JSON report
$ ./multi-llm-review --config config.yaml --format json /path/to/codebase

# Specify custom output directory
$ ./multi-llm-review --config config.yaml --output ./audit-results /path/to/codebase
```

---

## Validation

Generated reports must:
- Be valid Markdown or JSON
- Include all required sections
- Sort issues by severity and consensus
- Attribute issues to providers correctly
- Be human-readable and machine-parseable
